{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e611373c11947f6bade866741fd7670",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 12:59:52,882 - INFO - PyTorch version 2.4.1+cu124 available.\n",
      "2025-03-02 12:59:52,882 - INFO - TensorFlow version 2.10.1 available.\n",
      "2025-03-02 12:59:54,776 - INFO - Iniciando pipeline de preprocesamiento...\n",
      "2025-03-02 12:59:54,776 - INFO - ðŸ“Š 1. Carga de datos.\n",
      "2025-03-02 12:59:54,781 - INFO - \tCargando datos desde: D:\\OneDrive - Universidad de La Salle\\MaestrÃ­a IA\\S4\\Desarrollo de soluciones\\Proyecto\\Repo desarrollo soluciones\\data\\raw\\clasificacion_siniestros.csv\n",
      "2025-03-02 12:59:55,550 - INFO - âœ… Datos cargados: 63164 filas y 66 columnas.\n",
      "2025-03-02 12:59:55,560 - INFO - ðŸ§¹ 2. Limpieza de datos.\n",
      "2025-03-02 12:59:55,571 - INFO - \tIniciando limpieza: (63164, 66) registros.\n",
      "2025-03-02 12:59:55,904 - INFO - \tColumnas eliminadas por alto porcentaje de nulos (>50%): ['id_act_economica_igdacmlmasolicitudes', 'fecha_muerte_igatepmafurat', 'muerte_posterior_igatepmafurat', 'fecha_aviso_muerte_igatepmafurat']\n",
      "2025-03-02 12:59:55,992 - INFO - \tTotal registros eliminados por outliers: 2644.\n",
      "2025-03-02 12:59:57,218 - INFO - \tLimpieza completada: (54638, 22) registros (cambio de (63164, 66) a (54638, 22)).\n",
      "2025-03-02 12:59:57,227 - INFO - âœ… Proceso de limpieza finalizado.\n",
      "2025-03-02 12:59:57,227 - INFO - ðŸ”„ 3. TransformaciÃ³n de datos.\n",
      "2025-03-02 12:59:57,268 - INFO - \tSe han extraÃ­do las variables temporales 'mes' y 'dÃ­a' del siniestro.\n",
      "2025-03-02 12:59:57,269 - INFO - \tSe han remapeado los valores de la columna 'ind_realizando_trabajo_hab_at_igatepmafurat'.\n",
      "2025-03-02 12:59:57,553 - INFO - \tSe han ajustado las columnas de categorÃ­a.\n",
      "2025-03-02 12:59:57,886 - INFO - \tSe han capturado las periodicidades en las variables temporales con seno y coseno.\n",
      "2025-03-02 12:59:57,914 - INFO - \tSe han imputado los espacios vacÃ­os o ceros en variables binarias.\n",
      "2025-03-02 12:59:57,920 - INFO - âœ… Proceso de transformaciÃ³n finalizado.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "from data_loader import load_data\n",
    "from data_cleaning import clean_data\n",
    "from data_transformation import transform_data\n",
    "from feature_engineering import transform_and_split_data\n",
    "import pandas as pd\n",
    "import scipy.sparse\n",
    "from pathlib import Path\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "OUTPUT_DIR = Path().resolve().parent.parent.joinpath(\"data\", \"processed\")\n",
    "\n",
    "def save_data(X_train, X_val, X_test, y_train, y_val, y_test, output_dir=OUTPUT_DIR):\n",
    "    # Guarda las variables objetivo (y) como CSV\n",
    "    y_train.to_csv(f\"{output_dir}/y_train.csv\", index=False)\n",
    "    y_val.to_csv(f\"{output_dir}/y_val.csv\", index=False)\n",
    "    y_test.to_csv(f\"{output_dir}/y_test.csv\", index=False)\n",
    "\n",
    "    # Guarda los features (X) como Parquet o CSV\n",
    "    if isinstance(X_train, scipy.sparse.spmatrix):\n",
    "        scipy.sparse.save_npz(f\"{output_dir}/X_train.npz\", X_train)\n",
    "        scipy.sparse.save_npz(f\"{output_dir}/X_val.npz\", X_val)\n",
    "        scipy.sparse.save_npz(f\"{output_dir}/X_test.npz\", X_test)\n",
    "    else:\n",
    "        pd.DataFrame(X_train).to_parquet(f\"{output_dir}/X_train.parquet\")\n",
    "        pd.DataFrame(X_val).to_parquet(f\"{output_dir}/X_val.parquet\")\n",
    "        pd.DataFrame(X_test).to_parquet(f\"{output_dir}/X_test.parquet\")\n",
    "\n",
    "    logging.info(f\"âœ… Datos guardados en {output_dir}\")\n",
    "\n",
    "\n",
    "# def run_data_preprocessing_pipeline()\n",
    "\n",
    "logging.info(\"Iniciando pipeline de preprocesamiento...\")\n",
    "\n",
    "logging.info(f\"ðŸ“Š 1. Carga de datos.\")\n",
    "data = load_data(\"clasificacion_siniestros.csv\")\n",
    "logging.info(f\"âœ… Datos cargados: {data.shape[0]} filas y {data.shape[1]} columnas.\")\n",
    "\n",
    "logging.info(f\"ðŸ§¹ 2. Limpieza de datos.\")\n",
    "data = clean_data(data.copy())\n",
    "logging.info(f\"âœ… Proceso de limpieza finalizado.\")\n",
    "\n",
    "\n",
    "logging.info(f\"ðŸ”„ 3. TransformaciÃ³n de datos.\")\n",
    "data = transform_data(data.copy())\n",
    "logging.info(f\"âœ… Proceso de transformaciÃ³n finalizado.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-02 13:00:01,802 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "2025-03-02 13:00:05,140 - INFO - Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9218c2adbf461d8e2a0ec1d3db7933",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1196 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f6a21ae93534688807a8215994a5e73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/171 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22bf72b8287a4a5aa79220479375a961",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/342 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "class HighCardinalityEncoder(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, high_cardinality_cols):\n",
    "        self.high_cardinality_cols = high_cardinality_cols\n",
    "        self.mappings = {}\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for col in self.high_cardinality_cols:\n",
    "            self.mappings[col] = X[col].value_counts(normalize=True).to_dict()\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        for col in self.high_cardinality_cols:\n",
    "            X[col + '_freq'] = X[col].map(lambda x: self.mappings[col].get(x, 0))\n",
    "        return X.drop(columns=self.high_cardinality_cols)\n",
    "\n",
    "class TextEmbeddingTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, model_name='all-MiniLM-L6-v2'):\n",
    "        self.model_name = model_name\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        self.model = SentenceTransformer(model_name, device=self.device)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Asegurar que X es una Serie (columna Ãºnica)\n",
    "        if isinstance(X, pd.DataFrame):\n",
    "            X = X.iloc[:, 0]\n",
    "\n",
    "        X = X.fillna('missing').astype(str)\n",
    "\n",
    "        try:\n",
    "            embeddings = self.model.encode(\n",
    "                X.tolist(),\n",
    "                convert_to_numpy=True,\n",
    "                device=self.device,\n",
    "                batch_size=32  # Ajustar el tamaÃ±o de batch si es necesario\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error al procesar embeddings: {e}\")\n",
    "            embeddings = np.zeros((len(X), self.model.get_sentence_embedding_dimension()))\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "def detect_column_types(df, target_col, high_cardinality_threshold=20):\n",
    "    text_col = \"descripcion_at_igatepmafurat\"\n",
    "\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    numerical_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "    exclude_cols = {text_col, target_col}\n",
    "    categorical_cols = [col for col in categorical_cols if col not in exclude_cols]\n",
    "\n",
    "    high_cardinality_cols = [col for col in categorical_cols if df[col].nunique() > high_cardinality_threshold]\n",
    "\n",
    "    categorical_cols = [col for col in categorical_cols if col not in high_cardinality_cols]\n",
    "\n",
    "    return numerical_cols, categorical_cols, high_cardinality_cols, text_col\n",
    "\n",
    "def create_feature_engineering_pipeline(df):\n",
    "    numerical_cols, categorical_cols, high_cardinality_cols, text_col = detect_column_types(df, target_col='origen_igdactmlmacalificacionorigen')\n",
    "\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "\n",
    "    high_cardinality_transformer = Pipeline(steps=[\n",
    "        ('high_cardinality', HighCardinalityEncoder(high_cardinality_cols))\n",
    "    ])\n",
    "\n",
    "    text_transformer = Pipeline(steps=[\n",
    "        ('embedder', TextEmbeddingTransformer())\n",
    "    ])\n",
    "\n",
    "    feature_engineering = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numerical_cols),\n",
    "            ('cat', categorical_transformer, categorical_cols),\n",
    "            ('high_card', high_cardinality_transformer, high_cardinality_cols),\n",
    "            ('text', text_transformer, [text_col]),\n",
    "        ],\n",
    "        remainder='drop'\n",
    "    )\n",
    "\n",
    "    preprocessing_pipeline = Pipeline(steps=[\n",
    "        ('features', feature_engineering)\n",
    "    ])\n",
    "\n",
    "    return preprocessing_pipeline\n",
    "\n",
    "def split_data(df, target_column, test_size=0.2, val_size=0.1, random_state=42):\n",
    "    train_df, test_df = train_test_split(df, test_size=test_size, random_state=random_state)\n",
    "    train_df, val_df = train_test_split(train_df, test_size=val_size / (1 - test_size), random_state=random_state)\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def transform_and_split_data(df, target_column='origen_igdactmlmacalificacionorigen'):\n",
    "    train_df, val_df, test_df = split_data(df, target_column)\n",
    "\n",
    "    X_train, y_train = train_df.drop(columns=[target_column]), train_df[target_column]\n",
    "    X_val, y_val = val_df.drop(columns=[target_column]), val_df[target_column]\n",
    "    X_test, y_test = test_df.drop(columns=[target_column]), test_df[target_column]\n",
    "\n",
    "    pipeline = create_feature_engineering_pipeline(X_train)\n",
    "\n",
    "    X_train_transformed = pipeline.fit_transform(X_train)\n",
    "    X_val_transformed = pipeline.transform(X_val)\n",
    "    X_test_transformed = pipeline.transform(X_test)\n",
    "\n",
    "    return X_train_transformed, y_train, X_val_transformed, y_val, X_test_transformed, y_test\n",
    "\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = transform_and_split_data(data.copy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38246, 487)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "logging.info(f\"ðŸ”§ 4. IngenierÃ­a de caracterÃ­sticas y particiÃ³n de datos\")\n",
    "x_train, y_train, x_val, y_val, x_test, y_test = transform_and_split_data(data.copy())\n",
    "logging.info(f\"âœ… Proceso de ingenierÃ­a de caracterÃ­sticas y particiÃ³n de datos finalizado.\")\n",
    "\n",
    "logging.info(f\"ðŸ“¦ 5. Guardando datos procesados.\")\n",
    "save_data(x_train, x_val, x_test, y_train, y_val, y_test)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_win",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
